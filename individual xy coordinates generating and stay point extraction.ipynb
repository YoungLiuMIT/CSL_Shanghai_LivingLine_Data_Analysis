{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step1: use the exploreWifiLL.ipynb to generate the individual mobility data for each peoriod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import urllib\n",
    "import re\n",
    "import datetime\n",
    "import pytz\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import operator\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Folder_Path = r'E:\\jupyter3\\raw data'\n",
    "os.chdir(Folder_Path)\n",
    "with open ('2018-07-28_05','r', encoding='utf-8',errors='replace') as myfile:\n",
    "    data = myfile.readlines()\n",
    "    dataRawJson=[json.loads(data[i]) for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('pri-2018-07-28_05','r',encoding='utf-8',errors='replace') as myfile:\n",
    "    data = myfile.readlines()\n",
    "dataFormatted = '['+re.sub('}\\s*{','},{',data[0])+']' \n",
    "dataCleanJson=json.loads(dataFormatted)\n",
    "dataCleanJsonLists=[[rec['data'][i].split(',')for i in range(len(rec['data']))]for rec in dataCleanJson]\n",
    "validMacs = [[record[i][5] for i in range(len(record))] for record in dataCleanJsonLists]\n",
    "uniqueValidMacs=set([mac for macList in validMacs for mac in macList])\n",
    "numUniqueValidMacs = len(uniqueValidMacs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeCorrection=8*60*60*1000\n",
    "allTimes=[]\n",
    "individuals={}\n",
    "c=0\n",
    "for record in dataRawJson:\n",
    "    c+=1\n",
    "    wifi=record['wifidata']\n",
    "    apmac=wifi['apmac']\n",
    "    ts=float(wifi['tssend'])+timeCorrection \n",
    "    allTimes.extend([ts])\n",
    "    dateTime=datetime.datetime.utcfromtimestamp(ts/1000).strftime('%Y%m%d %H:%M:%S') \n",
    "    count=wifi['count']\n",
    "    for obs in wifi['wifitalist']:\n",
    "        valid=obs['mac'] in uniqueValidMacs\n",
    "        newObs={'probe':apmac,\n",
    "                'ts':ts,\n",
    "                'datetime':dateTime,\n",
    "                'rssi':obs['rssi']} \n",
    "        if obs['mac'] in individuals:\n",
    "            individuals[obs['mac']]['observations'].append(newObs)\n",
    "        else:\n",
    "            individuals[obs['mac']]={'valid':valid,'observations':[newObs]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step2: covert the json format data into csv format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('indi072805.json', 'w') as outfile: \n",
    "    json.dump(individuals, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Folder_Path = r'E:\\jupyter3\\raw data' #your own filepath\n",
    "os.chdir(Folder_Path)\n",
    "with open ('indi072805.json','r', encoding='utf-8',errors='replace') as myfile:\n",
    "    data = myfile.readlines()\n",
    "    IndividualJson=[json.loads(data[i]) for i in range(len(data))]\n",
    "jsondata = IndividualJson[0]\n",
    "key = list(IndividualJson[0].keys())\n",
    "idx=0\n",
    "result= json_normalize(IndividualJson[0][key[0]],'observations')\n",
    "result.insert(loc=idx,column='id',value=key[0])\n",
    "for i in range(1, len(key)):\n",
    "    test = json_normalize(IndividualJson[0][key[i]],'observations')\n",
    "    test.insert(loc=idx,column='id',value=key[i])\n",
    "    result = result.append(test,ignore_index=True)\n",
    "#print(result)\n",
    "result.to_csv('indi072805.csv',sep=',',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge the six csvs into one, which is the data for one whole day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Folder_Path = r'E:\\jupyter3\\raw data\\0728_csv'\n",
    "SaveFile_Path =  r'E:\\jupyter3\\raw data\\0728_csv'\n",
    "SaveFile_Name = r'allindi_0728.csv'\n",
    "os.chdir(Folder_Path)\n",
    "file_list = os.listdir()\n",
    "df = pd.read_csv(Folder_Path +'\\\\'+ file_list[0]) \n",
    "df.to_csv(SaveFile_Path+'\\\\'+ SaveFile_Name,encoding=\"utf_8_sig\",index=False)\n",
    "for i in range(1,len(file_list)):\n",
    "    df = pd.read_csv(Folder_Path + '\\\\'+ file_list[i])\n",
    "    df.to_csv(SaveFile_Path+'\\\\'+ SaveFile_Name,encoding=\"utf_8_sig\",index=False, header=False, mode='a+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step3: parse the data and add xy coordinates to the probes; thus we can calculate the coordinates for each individual at a certain time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Folder_Path = r'E:\\jupyter3\\raw data\\csv'\n",
    "os.chdir(Folder_Path)\n",
    "indi0728 = pd.read_csv('all_indi0728.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apmac_info = pd.read_csv('apmac_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apmac_info is the records of the xy coordinates for each probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indi_final = indi0728.merge(apmac_info, on=\"probe\", how = 'inner').fillna(0)\n",
    "indi_final.sort_values('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scripts for how to calculate the coordinates for each individual will be added later; I conducted it via Stata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the data to the format of list for individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note:information for each individual: {'id': agent_id, 'x': list_of_x_coordinates, 'y': list_of_y_coordinates, 'timestamps': list_of_timestamps }\n",
    "then list for all the individuals:[agent1_dict,  agent2_dict, ....]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = pd.read_csv('0728id_xy_version2.csv')\n",
    "with open('0728id_xy_version2.csv') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    rows = list(reader)\n",
    "\n",
    "with open('0728id_xy_version2.json', 'w') as f:\n",
    "    json.dump(rows, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('0728id_xy_version2.json','r', encoding='utf-8',errors='replace') as myfile:\n",
    "    data = myfile.readlines()\n",
    "    ind_data=[json.loads(data[i]) for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('0728id_xy_version2.json') as f:\n",
    "    things = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_things = {}\n",
    "for thing in things:\n",
    "    thing_id = thing[0]['id']\n",
    "    try:\n",
    "        old_thing = new_things[thing_id]\n",
    "    except KeyError:\n",
    "        new_things[thing_id] = thing\n",
    "    else:\n",
    "        old_thing['x'].extend(thing['x'])\n",
    "        old_thing['y'].extend(thing['y'])\n",
    "        old_thing['timestamps'].extend(thing['timestamps'])\n",
    "new_things = new_things.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(things):\n",
    "    return {'id': things[0]['id'],\n",
    "            'x': list(itertools.chain(float(t['x']) for t in things)),\n",
    "            'y': list(itertools.chain(float(t['y']) for t in things)),\n",
    "            'timestamps': list(itertools.chain(float(t['timestamps']) for t in things))}\n",
    "sorted_things = sorted(things[0], key=operator.itemgetter('id'))\n",
    "grouped_things = itertools.groupby(sorted_things, key=operator.itemgetter('id'))\n",
    "agents = [merge(list(group)) for key, group in grouped_things]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def maxDistFromGroup(newPoint, pointGroup):\n",
    "    #takes a point and an arra of points and returns \n",
    "    #the max dist of any point in the group from this new point\n",
    "    maxDist=0\n",
    "    for p in pointGroup:\n",
    "        distN=np.sqrt( (p[1] - newPoint[1])**2 + (p[0] - newPoint[0])**2)\n",
    "        maxDist=max(distN,maxDist)\n",
    "    return maxDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxRoam=15\n",
    "minStay=20\n",
    "def extractStays(agents, maxRoam, minStay):\n",
    "    #takes a list of agent objects, and returns a list of stays for each agent\n",
    "    #each agent in the input data should be a dictionary with an id, a list of x coordinates, a list y coordinates and a list of timestamps\n",
    "    # the lists of coordintes and timestamps must also be sorted from earliest to latest\n",
    "    #maxRoam is the distance threshold and minStay is the time threshold\n",
    "    stayData=[]\n",
    "    for a in range(len(agents)):\n",
    "        xList, yList, timeList =agents[a]['x'], agents[a]['y'], agents[a]['timestamps']\n",
    "        stays=[]\n",
    "        i=0\n",
    "        while i<len(xList):\n",
    "            j=i\n",
    "            while (j+1)<len(xList) and maxDistFromGroup([xList[j+1],yList[j+1]], [[xList[ij],yList[ij]] for ij in range(i,j+1)])<maxRoam:\n",
    "                j+=1\n",
    "            #create a stay from the middle position with arrival and departure time\n",
    "            jj=min(j+1,len(xList)-1)\n",
    "            # departure time is the timestamp of the next obs after this cluster. Unless this cluster contains the last point in the series\n",
    "            if timeList[jj]-timeList[i]>(minStay):\n",
    "                stays.extend([{'p':[xList[int((i+j)/2)],yList[int((i+j)/2)]], 'start':timeList[i],'end':timeList[jj], 'n':(j-i+1)}])       \n",
    "            i=j+1\n",
    "        if stays: # if stays is empty, no need to add anything\n",
    "            #add the new agent to the stay data\n",
    "            stayData.append({'stays':stays, 'id': agents[a]['id']})\n",
    "    return stayData\n",
    "#here we set the maxRoam is 15 meters and the minStay is 20 minutes\n",
    "if __name__ == '__main__':\n",
    "    maxRoam=15000\n",
    "    minStay=20*60*1000\n",
    "    result = extractStays(agents, maxRoam, minStay)\n",
    "    print (result[0])\n",
    "#result is the stay point extracted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
